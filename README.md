# Implementation of Machine Learning Algorithm from Scratch
Learn Machine Learning from basic to advance and develop Machine Learning Models from Scratch in Python

## 1. WHAT YOU WILL LEARN?
* Obtain a solid understand of machine learning in general from basic to advance
* Complete tutorial about basic packages like NumPy and Pandas
* Data Preprocessing and Data Visualization
* Have an understand of Machine Learning and how to apply it in your own programs
* Understanding the concept behind the algorithms
* Knowing how to optimize hyperparameters of your models
* Learn how to develop models based on the requirement of your future business
* Potential for a new job in the future

## 2. DESCRIPTION
Are you interested in Data Science and Machine Learning, but you donâ€™t have any background, and you find the concepts confusing?
<br>Are you interested in programming in Python, but you always afraid of coding?

<b> ðŸ˜ŠI think this repo is for you!ðŸ˜Š</b>

Even you are familiar with machine learning, this repo can help you to review all the techniques and understand the concept behind each term.
This repo is completely categorized, and I donâ€™t start from the middle! I actually start the concept of every term, and then I try to implement it in Python step by step. The structure of the course is as follows:

## 3. WHO THIS REPO IS FOR:
* Anyone with any background that interested in Data Science and Machine Learning with at least high school (+2) knowledge in mathematics
* Beginners, intermediate, and even advanced students in the field of Artificial Intelligence (AI), Data Science (DS), and Machine Learning (ML)
* Students in college that looking for securing their future jobs
* Students that look forward to excel their Final Year Project by learning Machine Learning
* Anyone who afraid of coding in Python but interested in Machine Learning concepts
* Anyone who wants to create new knowledge on the different dataset using machine learning
* Students who want to apply machine learning models in their projects

## 4. Contents
* [Useful Commands](#useful-commands) 
* [Installation](#installation)
* [Reality vs Expectation](#reality-vs-expectation)
* [Machine Learning from Beginner to Advanced](#machine-learning-from-beginner-to-advanced)
* [Scratch Implementation](#scratch-implementation)
* [Mathematical Implementation](#mathematical-implementation)
* [Machine Learning Interview Questions with Answers](#machine-learning-interview-questions-with-answers)
* [Essential Machine Learning Formulas](#essential-machine-learning-formulas)
* [Pratice Guide for Data Science Learning](#pratice-guide-for-data-science-learning)

# Useful Resources
| Title | Repository |
|------ | :----------: |
| USEFUL GIT COMMANDS FOR EVERYDAY USE | [ðŸ”—](https://github.com/ghimiresunil/Git-Cheat-Sheet)|
| MOST USEFUL LINUX COMMANDS EVERYONE SHOULD KNOW | [ðŸ”—](https://github.com/ghimiresunil/Linux-Guide-For-All)|
| AWESOME ML TOOLBOX| [ðŸ”—](https://github.com/ghimiresunil/Awesome-ML-Toolbox)|

# Installation
| Title | Repository |
|------ | :----------: |
|INSTALL THE ANACONDA PYTHON ON WINDOWS AND LINUX | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Installation/install_anaconda_python.md)|

# Reality vs Expectation
| Title | Repository |
|------ | :----------: |
| IS AI OVERHYPED? REALITY VS EXPECTATION |[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Reality%20vs%20Expectation/Is%20AI%20Overhyped%3F%20Reality%20vs%20Expectation.md)|

# Machine Learning from Beginner to Advanced
| Title | Repository |
|------ | :----------: |
|HISTORY OF MATHEMATICS, AI & ML - HISTORY & MOTIVATION| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/mathematics_ai_ml_history_motivation.md)|
| INTRODUCTION TO ARTIFICIAL INTELLIGENCE & MACHINE LEARNING |[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Introduction%20to%20ML%20and%20AI.md)|
| KEY TERMS USED IN MACHINE LEARNING | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Key%20terms%20used%20in%20ML.md) |
|PERFORMANCE METRICS IN MACHINE LEARNING CLASSIFICATION MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Classification%20Performance%20Metrics.md) |
|PERFORMANCE METRICS IN MACHINE LEARNING REGRESSION MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Machine%20Learning%20from%20Beginner%20to%20Advanced/Regression%20Performance%20Metrics.md) |

# Scratch Implementation
| Title | Repository |
|------ | :----------: |
|LINEAR REGRESSION FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Linear%20Regression)|
|LOGISTIC REGRESSION FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Logistic%20Regression)|
|NAIVE BAYES FROM SCRATCH| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Naive%20Bayes)|
|DECISION TREE FROM SCRATCH|[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/ML%20from%20Scratch/Decision%20Tree/README.md)|
|RANDOM FOREST FROM SCRATCH|[ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Random%20Forest)|
| K NEAREST NEIGHBOUR | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/KNN)|
| NAIVE BAYES | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/Naive%20Bayes)|
| K MEANS CLUSTERING | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/tree/main/ML%20from%20Scratch/K%20Means%20Clustering)|

# Mathematical Implementation
| Title | Repository |
|------ | :----------: |
|CONFUSION MATRIX FOR YOUR MULTI-CLASS ML MODEL| [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Mathematical%20Implementation/confusion_matrix.md)|

# Machine Learning Interview Questions with Answers
| Title | Repository |
|------ | :----------: |
|50 QUESTIONS ON STATISTICS & MACHINE LEARNING â€“ CAN YOU ANSWER? | [ðŸ”—](https://graspcoding.com/50-questions-on-statistics-machine-learning-can-you-answer/)|

# Essential Machine Learning Formulas
| Title | Repository |
|------ | :----------: |
|MOSTLY USED MACHINE LEARNING FORMULAS |[ðŸ”—](https://github.com/ghimiresunil/Machine-Learning-Formulas)|

# Pratice Guide for Data Science Learning
| Title | Repository |
|------ | :----------: |
| Research Guide for FYP | [ðŸ”—](https://github.com/ghimiresunil/Implementation-of-Machine-Learning-Algorithm-from-Scratch/blob/main/Pratice%20Guide/research_guide_for_fyp.md)|
|The Intermediate Guide to 180 Days Data Science Learning Plan|[ðŸ”—](https://graspcoding.com/the-intermediate-guide-to-180-days-data-science-learning-plan/)|

*** 

### Algorithm Pros and Cons

- KN Neighbors \
   âœ” Simple, No training, No assumption about data, Easy to implement, New data can be added seamlessly, Only one hyperparameter \
   âœ– Doesn't work well in high dimensions, Sensitive to noisy data, missing values and outliers, Doesn't work well with large data sets â€”  cost of calculating distance is high, Needs feature scaling, Doesn't work well on imbalanced data, Doesn't deal well with missing values

- Decision Tree \
   âœ” Doesn't require standardization or normalization, Easy to implement, Can handle missing values, Automatic feature selection \
   âœ– High variance, Higher training time, Can become complex, Can easily overfit

- Random Forest \
   âœ” Left-out data can be used for testing, High accuracy, Provides feature importance estimates, Can handle missing values, Doesn't require feature scaling, Good performance on imbalanced datasets, Can handle large dataset, Outliers have little impact, Less overfitting \
   âœ– Less interpretable, More computational resources, Prediction time high

- Linear Regression \
   âœ” Simple, Interpretable, Easy to Implement \
   âœ– Assumes linear relationship between features, Sensitive to outliers

- Logistic Regression \
   âœ” Doesnâ€™t assume linear relationship between independent and dependent variables, Output can be interpreted as probability, Robust to noise \
   âœ– Requires more data, Effective when linearly separable

- Lasso Regression (L1) \
   âœ” Prevents overfitting, Selects features by shrinking coefficients to zero \
   âœ– Selected features will be biased, Prediction can be worse than Ridge

- Ridge Regression (L2) \
   âœ” Prevents overfitting \
   âœ– Increases bias, Less interpretability 

- AdaBoost \
   âœ” Fast, Reduced bias, Little need to tune \
   âœ– Vulnerable to noise, Can overfit

- Gradient Boosting \
   âœ” Good performance \
   âœ– Harder to tune hyperparameters

- XGBoost \
   âœ” Less feature engineering required, Outliers have little impact, Can output feature importance, Handles large datasets, Good model performance, Less prone to overfitting \â€‹
   âœ– Difficult to interpret, Harder to tune as there are numerous hyperparameters

- SVM \
   âœ” Performs well in higher dimensions, Excellent when classes are separable, Outliers have less impact \
   âœ– Slow, Poor performance with overlapping classes, Selecting appropriate kernel functions can be tricky

- NaÃ¯ve Bayes \
   âœ” Fast, Simple, Requires less training data, Scalable, Insensitive to irrelevant features, Good performance with high-dimensional data \
   âœ– Assumes independence of features

- Deep Learning \
  âœ” Superb performance with unstructured data (images, video, audio, text) \
  âœ– (Very) long training time, Many hyperparameters, Prone to overfitting


***	

***
### AI/ML dataset

| Source | Link |
|------ | :----------: |
| Google Dataset Search â€“ A search engine for datasets: | [ðŸ”—](https://datasetsearch.research.google.com/) |
| IBMâ€™s collection of datasets for enterprise applications | [ðŸ”—](https://developer.ibm.com/exchanges/data/ ) |
| Kaggle Datasets | [ðŸ”—](https://www.kaggle.com/datasets) |
| Huggingface Datasets â€“ A Python library for loading NLP datasets | [ðŸ”—](https://github.com/huggingface/datasets) |
| A large list organized by application domain | [ðŸ”—](https://github.com/awesomedata/awesome-public-datasets) |
| Computer Vision Datasets (a really large list) | [ðŸ”—](https://homepages.inf.ed.ac.uk/rbf/CVonline/Imagedbase.htm) |
| Datasetlist â€“ Datasets by domain | [ðŸ”—](https://www.datasetlist.com/) |
| OpenML â€“ A search engine for curated datasets and workflows| [ðŸ”—](https://www.openml.org/search?type=data ) |
| Papers with Code â€“ Datasets with benchmarks | [ðŸ”—](https://www.paperswithcode.com/datasets) |
| Penn Machine Learning Benchmarks | [ðŸ”—](https://github.com/EpistasisLab/pmlb/tree/master/datasets) |
| VisualDataDiscovery (for Computer Vision) | [ðŸ”—](https://www.visualdata.io/discovery) |
| UCI Machine Learning Repository | [ðŸ”—](https://archive.ics.uci.edu/ml/index.php) |
| Roboflow Public Datasets for computer vision | [ðŸ”—](https://public.roboflow.com/) |
***
